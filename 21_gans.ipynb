{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs (Generative Adversarial Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modeling\n",
    "num_input = 28 * 28\n",
    "num_hidden = 256\n",
    "num_noise = 128 # 생성기의 입력값으로 사용할 노이즈의 크기\n",
    "\n",
    "# Learning\n",
    "total_epoch = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Evaluation\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight와 bias를 정의해 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs도 unsupervised learning의 하나이기 때문에 label이 없으므로 y_true 필요 없습니다.\n",
    "\n",
    "단, noise z_true를 입력값으로 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_true = tf.placeholder(tf.float32, [None, num_input])\n",
    "z_true = tf.placeholder(tf.float32, [None, num_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator NN 에 사용하는 변수들을 정의해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_generator_1 = tf.Variable(tf.truncated_normal([num_noise, num_hidden], stddev=0.01))\n",
    "bias_generator_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "\n",
    "weight_generator_2 = tf.Variable(tf.truncated_normal([num_hidden, num_input], stddev=0.01))\n",
    "bias_generator_2 = tf.Variable(tf.zeros([num_input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator NN에 사용하는 변수들을 정의해 줍니다. \n",
    "\n",
    "이때 Discriminator의 최종 결과값은 진짜와 얼마나 가깝느냐를 판단하기 위한 1개의 scalar value임에 주목합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_discriminator_1 = tf.Variable(tf.truncated_normal([num_input, num_hidden], stddev=0.01))\n",
    "bias_discriminator_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "\n",
    "weight_discriminator_2 = tf.Variable(tf.truncated_normal([num_hidden, 1], stddev=0.01)) # 1임에 주목!\n",
    "bias_discriminator_2 = tf.Variable(tf.zeros([1])) # 1임에 주목!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator NN을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(noise_z):\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(noise_z, weight_generator_1) + bias_generator_1)\n",
    "    output_generator = tf.sigmoid(tf.matmul(hidden_layer, weight_generator_2) + bias_generator_2)\n",
    "    return output_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguisher NN을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(inputs):\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(inputs, weight_discriminator_1) + bias_discriminator_1)\n",
    "    output_discriminator = tf.sigmoid(tf.matmul(hidden_layer, weight_discriminator_2) + bias_discriminator_2)\n",
    "    return output_discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random noise (z_true) 를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noise(batch_size):\n",
    "    return np.random.normal(size=(batch_size, num_noise)) # (100,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노이즈를 이용해 랜덤한 이미지를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = generator(z_true) # (None, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노이즈를 이용해 생성한 이미지가 진짜 이미지인지 판별한 값을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_generator = discriminator(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "진짜 이미지를 이용해 판결한 값을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_real = discriminator(x_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GANs 모델의 최적화는 loss_G와 loss_D를 최대화하는 것입니다.\n",
    "* loss_D를 최대화하기 위해서는 D_generator를 최소화하게 됩니다. \n",
    "* Discriminator에 진짜 이미지를 넣었을 때에도 최대값을 tf.log(D_real), 가짜 이미지를 넣었을 때에도 최대값을 tf.log(1-D_generator) 을 갖도록 학습시키기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator는 Generator가 만들어낸 이미지가 가짜라고 판단하도록 discriminator NN을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면, loss_G를 최대화하기 위해서는 D_generator 값을 최대화하게 되는데, 이것은 가짜 이미지를 넣었을 때 discriminator가 최대한 실제 이미지라고 판단하도록 generator NN을 학습시킵니다. 논문에서는 loss_D와 같은 수식으로 최소화하는 generator를 찾지만, 결국 D_generator 값을 최대화하는 것이므로 다음과 같이 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_G = tf.reduce_mean(tf.log(D_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_D를 구할 때에는 generator NN에 사용되는 변수만 사용하여 최적화를 하고, loss_G를 구할 때에는 discriminator NN에 사용되는 변수만 사용하여 최적화를 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_var_list = [weight_discriminator_1, bias_discriminator_1, weight_discriminator_2, bias_discriminator_2]\n",
    "G_var_list = [weight_generator_1, bias_generator_1, weight_generator_2, bias_generator_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs 논문의 수식에 따르면 loss를 극대화해야 하지만, minimize하는 최적화함수를 사용하기 때문에 최적화하려는 loss_D와 loss_G에 음수 부호를 붙여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batch = int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_value_Discriminator = 0\n",
    "loss_value_Generator = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Loss (Discriminator) : -1.9008198251e-06, Loss (Generator) : -15.3966360092\n",
      "Epoch : 2, Loss (Discriminator) : -2.17089927901e-06, Loss (Generator) : -16.0739517212\n",
      "Epoch : 3, Loss (Discriminator) : -4.69089911803e-07, Loss (Generator) : -16.3357448578\n",
      "Epoch : 4, Loss (Discriminator) : -2.8553527045e-06, Loss (Generator) : -16.9084358215\n",
      "Epoch : 5, Loss (Discriminator) : -2.9385171274e-07, Loss (Generator) : -17.3470153809\n",
      "Epoch : 6, Loss (Discriminator) : -5.53731865693e-07, Loss (Generator) : -17.641412735\n",
      "Epoch : 7, Loss (Discriminator) : -2.53928442362e-06, Loss (Generator) : -18.0438404083\n",
      "Epoch : 8, Loss (Discriminator) : -2.11596855593e-07, Loss (Generator) : -16.8216533661\n",
      "Epoch : 9, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693133115768\n",
      "Epoch : 10, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 11, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 12, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 13, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 14, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 15, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 16, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 17, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 18, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 19, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 20, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 21, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 22, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 23, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 24, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 25, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 26, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 27, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 28, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 29, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n",
      "Epoch : 30, Loss (Discriminator) : -1.38629484177, Loss (Generator) : -0.693147420883\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size)\n",
    "\n",
    "        _, loss_value_Discriminator = sess.run([train_D, loss_D], {x_true:batch_xs,z_true:noise})\n",
    "        _, loss_value_Generator = sess.run([train_G, loss_G], {z_true:noise})\n",
    "    print \"Epoch : {0}, Loss (Discriminator) : {1}, Loss (Generator) : {2}\"\\\n",
    "            .format(epoch + 1, loss_value_Discriminator, loss_value_Generator)\n",
    "    \n",
    "    # 학습 과정을 보여주기 위해 주기적으로 이미지를 생성하여 저장합니다.\n",
    "    noise = get_noise(num_samples)\n",
    "    samples = sess.run(G, {z_true: noise})\n",
    "    fig, ax = plt.subplots(1, num_samples, figsize=(num_samples,1))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        ax[i].set_axis_off\n",
    "        ax[i].imshow(np.reshape(samples[i], (28,28)))\n",
    "        \n",
    "    plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
